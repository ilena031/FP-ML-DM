{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13988488,"sourceType":"datasetVersion","datasetId":8916126},{"sourceId":14051518,"sourceType":"datasetVersion","datasetId":8933140},{"sourceId":14054083,"sourceType":"datasetVersion","datasetId":8946004}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:34:14.881234Z","iopub.execute_input":"2025-12-06T12:34:14.881414Z","iopub.status.idle":"2025-12-06T12:34:20.426610Z","shell.execute_reply.started":"2025-12-06T12:34:14.881397Z","shell.execute_reply":"2025-12-06T12:34:20.425973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision transformers pillow pandas tqdm scikit-learn umap-learn plotly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:34:20.428180Z","iopub.execute_input":"2025-12-06T12:34:20.428517Z","iopub.status.idle":"2025-12-06T12:35:36.130688Z","shell.execute_reply.started":"2025-12-06T12:34:20.428499Z","shell.execute_reply":"2025-12-06T12:35:36.129790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import ViTImageProcessor, ViTModel\nfrom sklearn.manifold import TSNE\nfrom torchvision import transforms\nimport umap\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:35:36.131779Z","iopub.execute_input":"2025-12-06T12:35:36.132066Z","iopub.status.idle":"2025-12-06T12:36:15.989524Z","shell.execute_reply.started":"2025-12-06T12:35:36.132039Z","shell.execute_reply":"2025-12-06T12:36:15.988706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/input/suku-indonesia-new/Dataset_Final_Split/Train\"\nTEST_DIR = \"/kaggle/input/suku-indonesia-new/Dataset_Final_Split/Test\"\nTRAIN_OUTPUT = \"train_embeddings.csv\"\nTEST_OUTPUT = \"test_embeddings.csv\"\n\nMODEL_NAME = \"google/vit-base-patch16-224-in21k\"\nEMBEDDING_DIM = 768","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:36:15.990447Z","iopub.execute_input":"2025-12-06T12:36:15.991163Z","iopub.status.idle":"2025-12-06T12:36:15.995378Z","shell.execute_reply.started":"2025-12-06T12:36:15.991133Z","shell.execute_reply":"2025-12-06T12:36:15.994737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load VIT Pre-Trained","metadata":{}},{"cell_type":"code","source":"processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\nmodel = ViTModel.from_pretrained(MODEL_NAME)\n\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n    \nmodel = model.to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:36:15.996205Z","iopub.execute_input":"2025-12-06T12:36:15.996473Z","iopub.status.idle":"2025-12-06T12:36:18.230089Z","shell.execute_reply.started":"2025-12-06T12:36:15.996455Z","shell.execute_reply":"2025-12-06T12:36:18.229474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyper Paarameter Tuning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\n\nclass ImageDataset(Dataset):\n    def __init__(self, image_paths, labels, processor, augment=False):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.processor = processor\n        self.augment = augment\n\n        # label mapping\n        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n\n        # AUGMENTATION \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        if self.augment:\n            image = self.transform(image)\n\n        # resize after augmentasi\n        image = image.resize((224, 224), Image.Resampling.LANCZOS)\n\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        pixel_values = inputs['pixel_values'].squeeze(0)\n        label = self.label_to_idx[self.labels[idx]]\n        return pixel_values, label\n\ndef prepare_data(train_dir):\n    train_path = Path(train_dir)\n    image_paths = []\n    labels = []\n    \n    for class_folder in sorted(train_path.iterdir()):\n        if class_folder.is_dir():\n            class_name = class_folder.name\n            for img_file in class_folder.glob(\"*\"):\n                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n                    image_paths.append(str(img_file))\n                    labels.append(class_name)\n    \n    return train_test_split(image_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    best_val_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(pixel_values=inputs).logits\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_total += labels.size(0)\n            train_correct += predicted.eq(labels).sum().item()\n        \n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(pixel_values=inputs).logits\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * train_correct / train_total\n        val_acc = 100. * val_correct / val_total\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n        \n        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%\")\n    \n    return best_val_acc\n\ndef objective(trial):\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n    num_epochs = trial.suggest_int('num_epochs', 3, 10)\n    \n    train_imgs, val_imgs, train_lbls, val_lbls = prepare_data(TRAIN_DIR)\n    \n    processor_temp = ViTImageProcessor.from_pretrained(MODEL_NAME)\n    train_dataset = ImageDataset(train_imgs, train_lbls, processor_temp)\n    val_dataset = ImageDataset(val_imgs, val_lbls, processor_temp)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    num_labels = len(train_dataset.label_to_idx)\n    model = ViTModel.from_pretrained(MODEL_NAME)\n    model.config.num_labels = num_labels\n    \n    from transformers import ViTForImageClassification\n    model = ViTForImageClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, ignore_mismatched_sizes=True)\n    \n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    \n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    \n    best_val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n    \n    return best_val_acc\n\nprint(\"Starting Optuna hyperparameter tuning...\")\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"\\nBest trial: {study.best_trial.number}\")\nprint(f\"Best validation accuracy: {study.best_value:.2f}%\")\nprint(f\"Best hyperparameters: {study.best_params}\")\n\nbest_params = study.best_params\ntrain_imgs, val_imgs, train_lbls, val_lbls = prepare_data(TRAIN_DIR)\n\nprocessor = ViTImageProcessor.from_pretrained(MODEL_NAME)\ntrain_dataset = ImageDataset(train_imgs, train_lbls, processor)\nval_dataset = ImageDataset(val_imgs, val_lbls, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n\nnum_labels = len(train_dataset.label_to_idx)\n\nfrom transformers import ViTForImageClassification\nmodel = ViTForImageClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, ignore_mismatched_sizes=True)\n\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=best_params['lr'])\n\nprint(\"\\nTraining final model with best hyperparameters...\")\nfinal_acc = train_model(model, train_loader, val_loader, criterion, optimizer, best_params['num_epochs'], device)\n\ntorch.save(model.state_dict(), 'vit_finetuned.pth')\nprint(f\"\\nFinal model saved with validation accuracy: {final_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:36:18.230866Z","iopub.execute_input":"2025-12-06T12:36:18.231171Z","iopub.status.idle":"2025-12-06T13:09:27.266818Z","shell.execute_reply.started":"2025-12-06T12:36:18.231152Z","shell.execute_reply":"2025-12-06T13:09:27.265932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load FineTune","metadata":{}},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nprocessor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n\ntrain_imgs, val_imgs, train_lbls, val_lbls = prepare_data(TRAIN_DIR)\ntrain_dataset = ImageDataset(train_imgs, train_lbls, processor)\nnum_labels = len(train_dataset.label_to_idx)\n\nmodel = ViTForImageClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, ignore_mismatched_sizes=True)\n\nstate_dict = torch.load('vit_finetuned.pth')\nif list(state_dict.keys())[0].startswith('module.'):\n    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\nmodel.load_state_dict(state_dict)\n\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n    \nmodel = model.to(device)\nmodel.eval()\n\nprint(\"Fine-tuned model loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:09:27.269486Z","iopub.execute_input":"2025-12-06T13:09:27.269732Z","iopub.status.idle":"2025-12-06T13:09:28.541502Z","shell.execute_reply.started":"2025-12-06T13:09:27.269710Z","shell.execute_reply":"2025-12-06T13:09:28.540813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image: convert to RGB, resize, and normalize.\n    \n    Args:\n        image_path: Path to the image file\n        target_size: Target size tuple (width, height)\n        \n    Returns:\n        processed_image: PIL Image object ready for ViT processing\n    \"\"\"\n    try:\n        # Load image\n        image = Image.open(image_path)\n        \n        # Convert to RGB (handles RGBA, grayscale, etc.)\n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n            \n        # Resize to 224x224\n        image = image.resize(target_size, Image.Resampling.LANCZOS)\n        \n        # Convert to numpy array for normalization\n        img_array = np.array(image, dtype=np.float32)\n        \n        # Normalize pixel values from [0, 255] to ImageNet standards\n        # ImageNet mean: [0.485, 0.456, 0.406]\n        # ImageNet std: [0.229, 0.224, 0.225]\n        mean = np.array([0.485, 0.456, 0.406]) * 255\n        std = np.array([0.229, 0.224, 0.225]) * 255\n        \n        # Apply normalization: (pixel - mean) / std\n        img_array = (img_array - mean) / std\n        \n        # Convert back to PIL Image (ViTImageProcessor expects PIL Image)\n        # Scale back to 0-255 range for PIL\n        img_normalized = ((img_array * std + mean)).clip(0, 255).astype(np.uint8)\n        processed_image = Image.fromarray(img_normalized)\n        \n        return processed_image\n        \n    except Exception as e:\n        return None\n\n# Update the extract_embedding function to use preprocessing\ndef extract_embedding_with_preprocessing(image_path):\n    try:\n        processed_image = preprocess_image(image_path)\n        \n        if processed_image is None:\n            return None\n            \n        inputs = processor(images=processed_image, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            if hasattr(model, 'module'):\n                base_model = model.module.vit\n            else:\n                base_model = model.vit\n            outputs = base_model(**inputs)\n            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n        \n        return embedding\n    \n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:09:28.542237Z","iopub.execute_input":"2025-12-06T13:09:28.542514Z","iopub.status.idle":"2025-12-06T13:09:28.551859Z","shell.execute_reply.started":"2025-12-06T13:09:28.542494Z","shell.execute_reply":"2025-12-06T13:09:28.551233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Proces Train Set","metadata":{}},{"cell_type":"code","source":"def process_train_dataset(train_dir, output_file):\n    \"\"\"\n    Process train dataset and extract embeddings.\n    \n    Args:\n        train_dir: Path to train directory (contains class folders)\n        output_file: Path to save CSV file\n    \"\"\"\n    # Collect all image paths and labels\n    image_paths = []\n    labels = []\n    ids = []\n    \n    train_path = Path(train_dir)\n    \n    for class_folder in sorted(train_path.iterdir()):\n        if class_folder.is_dir():\n            class_name = class_folder.name\n            \n            for img_file in class_folder.glob(\"*\"):\n                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n                    image_paths.append(str(img_file))\n                    labels.append(class_name)\n                    ids.append(img_file.stem)\n    \n    # Extract embeddings\n    embeddings = []\n    valid_ids = []\n    valid_labels = []\n    \n    for img_path, img_id, label in tqdm(zip(image_paths, ids, labels), total=len(image_paths)):\n        embedding = extract_embedding_with_preprocessing(img_path)\n        if embedding is not None:\n            embeddings.append(embedding)\n            valid_ids.append(img_id)\n            valid_labels.append(label)\n    \n    # Create DataFrame\n    print(f\"\\nCreating DataFrame...\")\n    \n    # Create column names for embeddings\n    embed_cols = [f'embed_{i}' for i in range(EMBEDDING_DIM)]\n    \n    # Create DataFrame\n    df = pd.DataFrame(embeddings, columns=embed_cols)\n    embed_cols = [f'embed_{i}' for i in range(EMBEDDING_DIM)]\n    \n    df = pd.DataFrame(embeddings, columns=embed_cols)\n    df.insert(0, 'id', valid_ids)\n    df.insert(1, 'label', valid_labels)\n    \n    df.to_csv(output_file, index=False)\n    \n    return df\n\n# Process train dataset\ntrain_df = process_train_dataset(TRAIN_DIR, TRAIN_OUTPUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:09:28.552557Z","iopub.execute_input":"2025-12-06T13:09:28.552817Z","iopub.status.idle":"2025-12-06T13:10:00.827391Z","shell.execute_reply.started":"2025-12-06T13:09:28.552788Z","shell.execute_reply":"2025-12-06T13:10:00.826776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Process Test Set","metadata":{}},{"cell_type":"code","source":"def process_test_dataset(test_dir, output_file):\n    \"\"\"\n    Process test dataset and extract embeddings.\n    \n    Args:\n        test_dir: Path to test directory (flat structure, no subfolders)\n        output_file: Path to save CSV file\n    \"\"\"\n    \n    # Collect all image paths\n    test_path = Path(test_dir)\n    image_paths = []\n    ids = []\n    \n    for img_file in sorted(test_path.glob(\"*\")):\n        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n            image_paths.append(str(img_file))\n            ids.append(img_file.stem)\n    \n    # Extract embeddings\n    embeddings = []\n    valid_ids = []\n    \n    for img_path, img_id in tqdm(zip(image_paths, ids), total=len(image_paths)):\n        embedding = extract_embedding_with_preprocessing(img_path)\n        if embedding is not None:\n            embeddings.append(embedding)\n            valid_ids.append(img_id)\n    \n    # Create DataFrame\n    print(f\"\\nCreating DataFrame...\")\n    \n    # Create column names for embeddings\n    embed_cols = [f'embed_{i}' for i in range(EMBEDDING_DIM)]\n    \n    df = pd.DataFrame(embeddings, columns=embed_cols)\n    df.insert(0, 'id', valid_ids)\n    \n    df.to_csv(output_file, index=False)\n    \n    return df\n\n# Process test dataset\ntest_df = process_test_dataset(TEST_DIR, TEST_OUTPUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:10:00.828162Z","iopub.execute_input":"2025-12-06T13:10:00.828437Z","iopub.status.idle":"2025-12-06T13:10:11.184594Z","shell.execute_reply.started":"2025-12-06T13:10:00.828416Z","shell.execute_reply":"2025-12-06T13:10:11.183727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TSNE","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"train_embeddings.csv\")\nprint(df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:10:11.185498Z","iopub.execute_input":"2025-12-06T13:10:11.185759Z","iopub.status.idle":"2025-12-06T13:10:11.365937Z","shell.execute_reply.started":"2025-12-06T13:10:11.185734Z","shell.execute_reply":"2025-12-06T13:10:11.365326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_OUTPUT)\n\nX = train_data.iloc[:, 2:].values\nlabels = train_data['label'].values\n\ntsne = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000, verbose=0)\nX_tsne = tsne.fit_transform(X)\n\nvis_df = pd.DataFrame({\n    'x': X_tsne[:, 0],\n    'y': X_tsne[:, 1],\n    'z': X_tsne[:, 2],\n    'label': labels,\n    'id': train_data['id'].values\n})\n\n# Calculate centroids for each class\ncentroids = []\nunique_labels = sorted(vis_df['label'].unique())\n\nfor label in unique_labels:\n    class_data = vis_df[vis_df['label'] == label]\n    centroid = {\n        'x': class_data['x'].mean(),\n        'y': class_data['y'].mean(),\n        'z': class_data['z'].mean(),\n        'label': label,\n        'type': 'Centroid',\n        'id': f'{label}_centroid'\n    }\n    centroids.append(centroid)\n\ncentroid_df = pd.DataFrame(centroids)\n\n# Combine data points and centroids\nvis_df['type'] = 'Data Point'\ncombined_df = pd.concat([vis_df, centroid_df], ignore_index=True)\n\n# Calculate inter-class distances\nprint(\"\\nInter-class centroid distances:\")\nfor i, label1 in enumerate(unique_labels):\n    for label2 in unique_labels[i+1:]:\n        c1 = centroid_df[centroid_df['label'] == label1]\n        c2 = centroid_df[centroid_df['label'] == label2]\n        dist = np.sqrt(\n            (c1['x'].values[0] - c2['x'].values[0])**2 +\n            (c1['y'].values[0] - c2['y'].values[0])**2 +\n            (c1['z'].values[0] - c2['z'].values[0])**2\n        )\n        print(f\"  {label1} <-> {label2}: {dist:.2f}\")\n\n# Create visualization with centroids\nfig = px.scatter_3d(\n    combined_df,\n    x='x', y='y', z='z',\n    color='label',\n    symbol='type',\n    hover_data=['id'],\n    title='Train Embeddings with Centroids (t-SNE 3D)',\n    labels={'x': 't-SNE 1', 'y': 't-SNE 2', 'z': 't-SNE 3'},\n    opacity=0.6,\n    height=700\n)\n\n# Update marker sizes - larger points for better visibility\nfig.update_traces(\n    marker=dict(size=7, opacity=0.6),\n    selector=dict(mode='markers', name=lambda x: 'Data Point' in str(x))\n)\nfig.update_traces(\n    marker=dict(\n        size=35,\n        symbol='diamond',\n        line=dict(width=4, color='white'),\n        opacity=1.0\n    ),\n    selector=dict(mode='markers', name=lambda x: 'Centroid' in str(x))\n)\n\n# Add buttons to toggle classes\nbuttons = [{'label': 'All', 'method': 'update', 'args': [{'visible': [True] * len(fig.data)}]}]\n\nfor label in unique_labels:\n    visible = []\n    for trace in fig.data:\n        if label in trace.name:\n            visible.append(True)\n        else:\n            visible.append(False)\n    \n    buttons.append({\n        'label': label,\n        'method': 'update',\n        'args': [{'visible': visible}]\n    })\n\nfig.update_layout(\n    updatemenus=[{\n        'buttons': buttons,\n        'direction': 'down',\n        'showactive': True,\n        'x': 0.02,\n        'y': 0.98,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    }]\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:10:11.366763Z","iopub.execute_input":"2025-12-06T13:10:11.367025Z","iopub.status.idle":"2025-12-06T13:10:26.262053Z","shell.execute_reply.started":"2025-12-06T13:10:11.367006Z","shell.execute_reply":"2025-12-06T13:10:26.261429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LDA","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Apply LDA to reduce to 3 dimensions directly for visualization\nlda_3d = LinearDiscriminantAnalysis(n_components=3)\nX_lda_only = lda_3d.fit_transform(X, labels)\n\nprint(f\"LDA reduced dimensions: {X.shape[1]} -> {X_lda_only.shape[1]}\")\nprint(f\"Explained variance ratio: {lda_3d.explained_variance_ratio_}\")\nprint(f\"Total explained variance: {lda_3d.explained_variance_ratio_.sum():.4f}\")\n\nvis_df_lda_only = pd.DataFrame({\n    'x': X_lda_only[:, 0],\n    'y': X_lda_only[:, 1],\n    'z': X_lda_only[:, 2],\n    'label': labels,\n    'id': train_data['id'].values\n})\n\n# Calculate centroids for LDA only\ncentroids_lda_only = []\nfor label in unique_labels:\n    class_data = vis_df_lda_only[vis_df_lda_only['label'] == label]\n    centroid = {\n        'x': class_data['x'].mean(),\n        'y': class_data['y'].mean(),\n        'z': class_data['z'].mean(),\n        'label': label,\n        'type': 'Centroid',\n        'id': f'{label}_centroid'\n    }\n    centroids_lda_only.append(centroid)\n\ncentroid_df_lda_only = pd.DataFrame(centroids_lda_only)\n\n# Combine data and centroids\nvis_df_lda_only['type'] = 'Data Point'\ncombined_df_lda_only = pd.concat([vis_df_lda_only, centroid_df_lda_only], ignore_index=True)\n\n# Calculate inter-class distances for LDA only\nprint(\"\\nLDA inter-class centroid distances:\")\nfor i, label1 in enumerate(unique_labels):\n    for label2 in unique_labels[i+1:]:\n        c1 = centroid_df_lda_only[centroid_df_lda_only['label'] == label1]\n        c2 = centroid_df_lda_only[centroid_df_lda_only['label'] == label2]\n        dist = np.sqrt(\n            (c1['x'].values[0] - c2['x'].values[0])**2 +\n            (c1['y'].values[0] - c2['y'].values[0])**2 +\n            (c1['z'].values[0] - c2['z'].values[0])**2\n        )\n        print(f\"  {label1} <-> {label2}: {dist:.2f}\")\n\nfig_lda_only = px.scatter_3d(\n    combined_df_lda_only,\n    x='x', y='y', z='z',\n    color='label',\n    symbol='type',\n    hover_data=['id'],\n    title='Train Embeddings with Centroids (LDA 3D)',\n    labels={'x': 'LDA 1', 'y': 'LDA 2', 'z': 'LDA 3'},\n    opacity=0.6,\n    height=700\n)\n\nfig_lda_only.update_traces(\n    marker=dict(size=7, opacity=0.6),\n    selector=dict(mode='markers', name=lambda x: 'Data Point' in str(x))\n)\nfig_lda_only.update_traces(\n    marker=dict(\n        size=35,\n        symbol='diamond',\n        line=dict(width=4, color='white'),\n        opacity=1.0\n    ),\n    selector=dict(mode='markers', name=lambda x: 'Centroid' in str(x))\n)\n\n# Add buttons to toggle classes\nbuttons_lda_only = [{'label': 'All', 'method': 'update', 'args': [{'visible': [True] * len(fig_lda_only.data)}]}]\n\nfor label in unique_labels:\n    visible = []\n    for trace in fig_lda_only.data:\n        if label in trace.name:\n            visible.append(True)\n        else:\n            visible.append(False)\n    \n    buttons_lda_only.append({\n        'label': label,\n        'method': 'update',\n        'args': [{'visible': visible}]\n    })\n\nfig_lda_only.update_layout(\n    updatemenus=[{\n        'buttons': buttons_lda_only,\n        'direction': 'down',\n        'showactive': True,\n        'x': 0.02,\n        'y': 0.98,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    }]\n)\n\nfig_lda_only.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:24:45.903005Z","iopub.execute_input":"2025-12-06T13:24:45.903661Z","iopub.status.idle":"2025-12-06T13:24:46.428391Z","shell.execute_reply.started":"2025-12-06T13:24:45.903638Z","shell.execute_reply":"2025-12-06T13:24:46.427374Z"}},"outputs":[],"execution_count":null}]}